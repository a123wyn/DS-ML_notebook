{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import os\n",
    "import seaborn as sns#数据可视化\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle #用于存储模型\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "train = pd.read_excel(\"data1.xlsx\",index_col=u'企业代号')\n",
    "test = pd.read_excel(\"data2.xlsx\",index_col=u'企业代号')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 123 entries, E1 to E123\n",
      "Data columns (total 16 columns):\n",
      "id           123 non-null int64\n",
      "企业名称         123 non-null object\n",
      "信誉评级         123 non-null object\n",
      "是否违约         123 non-null object\n",
      "总销售额（含税）     123 non-null float64\n",
      "总销售额(不含税)    123 non-null float64\n",
      "发票状态         123 non-null object\n",
      "总票数          123 non-null int64\n",
      "废票数          123 non-null int64\n",
      "废票率          123 non-null float64\n",
      "销售利润率        123 non-null float64\n",
      "成本费用利润率      123 non-null float64\n",
      "需求不稳定性       123 non-null float64\n",
      "供给不稳定性       123 non-null float64\n",
      "种类           123 non-null object\n",
      "销售额增长率       123 non-null float64\n",
      "dtypes: float64(8), int64(3), object(5)\n",
      "memory usage: 16.3+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 302 entries, E124 to E425\n",
      "Data columns (total 13 columns):\n",
      "id           302 non-null int64\n",
      "企业名称         302 non-null object\n",
      "总销售额(不含税)    302 non-null float64\n",
      "发票状态         302 non-null object\n",
      "总票数          302 non-null int64\n",
      "废票数          302 non-null int64\n",
      "废票率          302 non-null float64\n",
      "销售利润率        302 non-null float64\n",
      "成本费用利润率      302 non-null float64\n",
      "需求不稳定性       302 non-null float64\n",
      "供给不稳定性       302 non-null float64\n",
      "种类           302 non-null object\n",
      "销售额增长率       302 non-null float64\n",
      "dtypes: float64(7), int64(3), object(3)\n",
      "memory usage: 33.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# 查看数据信息，date_first_booking缺失较多考虑删除\n",
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无空缺值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'first_affiliate_tracked'用出现最多的'untracked'补充缺失值\n",
    "# train.groupby('first_affiliate_tracked').count().sort_values('id',ascending=False)\n",
    "# # train['gender'].value_counts()，记录各种类数量\n",
    "# train['first_affiliate_tracked'].fillna('untracked',inplace=True)\n",
    "# test.groupby('first_affiliate_tracked').count().sort_values('id',ascending=False)\n",
    "# # test['gender'].value_counts()，记录各种类数量\n",
    "# test['first_affiliate_tracked'].fillna('untracked',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 25928 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 21457 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 31080 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 29366 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24577 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 25928 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 21457 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 31080 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 29366 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24577 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用柱状图统计各个特征情况\n",
    "def feature_barplot(feature, df_train = train, df_test = test, figsize=(10,5), rot = 90, saveimg = False): \n",
    "    feat_train = df_train[feature].value_counts()\n",
    "    feat_test = df_test[feature].value_counts()\n",
    "    fig_feature, (axis1,axis2) = plt.subplots(1,2,sharex=True, sharey = True, figsize = figsize)\n",
    "    sns.barplot(feat_train.index.values, feat_train.values, ax = axis1)\n",
    "    sns.barplot(feat_test.index.values, feat_test.values, ax = axis2)\n",
    "    axis1.set_xticklabels(axis1.xaxis.get_majorticklabels(), rotation = rot)\n",
    "    axis2.set_xticklabels(axis1.xaxis.get_majorticklabels(), rotation = rot)\n",
    "    axis1.set_title(feature + ' of training dataset')\n",
    "    axis2.set_title(feature + ' of test dataset')\n",
    "    axis1.set_ylabel('Counts')\n",
    "    plt.tight_layout()\n",
    "    if saveimg == True:\n",
    "        figname = feature + \".png\"\n",
    "        fig_feature.savefig(figname, dpi = 75)\n",
    "feature_barplot('发票状态')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simplify_ages(df):\n",
    "#     df['age'] = df['age'].fillna(-0.5)\n",
    "#     # 把Age分为不同区间,-1到0,0-3,3-12...,60及以上,放到bins里，八个区间，对应的八个区间名称在group_names那\n",
    "#     bins = [-1, 0, 10, 20, 30, 40, 50, 60, 120]\n",
    "#     group_names = ['Unknown', '[0,10)', '[10,20)', '[20,30)', '[30,40)', '[40,50)', '[50,60)','[60,120]']\n",
    "#     # 开始对数据进行离散化，pandas.cut就是这个功能\n",
    "#     catagories = pd.cut(df['age'], bins, labels=group_names,right=False)\n",
    "#     df['age'] = catagories\n",
    "#     #print(catagories.value_counts())\n",
    "#     return df\n",
    "# train=simplify_ages(train)\n",
    "# test=simplify_ages(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 重复值处理，无重复值\n",
    "# print(len(train['id'].unique()))\n",
    "# print(train['id'].count())\n",
    "# print(len(test['id'].unique()))\n",
    "# print(test['id'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 session文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = pd.read_csv('airbnb/sessions.csv')\n",
    "# session.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session['action'].isnull().sum()\n",
    "# session['action'].fillna('-unknown-',inplace=True)\n",
    "# session['action_type'].isnull().sum()\n",
    "# session['action_type'].fillna('-unknown-',inplace=True)\n",
    "# session['action_detail'].isnull().sum()\n",
    "# session['action_detail'].fillna('-unknown-',inplace=True)\n",
    "# session['secs_elapsed'].isnull().sum()/len(session['secs_elapsed'])\n",
    "# session['secs_elapsed'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 session文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更好合并\n",
    "# session['id'] = session['user_id']\n",
    "# del session['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对action特征进行细化\n",
    "# f_act = session['action'].value_counts().argsort() # argsort函数返回的是数组值从小到大的索引值\n",
    "# f_act_detail = session['action_detail'].value_counts().argsort()\n",
    "# f_act_type = session['action_type'].value_counts().argsort()\n",
    "# f_dev_type = session['device_type'].value_counts().argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照id进行分组\n",
    "# dgr_sess = session.groupby(['id'])\n",
    "# # Loop on dgr_sess to create all the features.\n",
    "# samples = []  # samples列表\n",
    "# ln = len(dgr_sess)  # 计算分组后df_sessions的长度\n",
    "# k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进度：100.00%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def session_feature():\n",
    "        global k\n",
    "        for g in dgr_sess:  # 对dgr_sess中每个id的数据进行遍历\n",
    "            gr = g[1]  # data frame that comtains all the data for a groupby value 'zzywmcn0jv'\n",
    "\n",
    "            l = []  # 建一个空列表，临时存放特征\n",
    "\n",
    "            # the id    for example:'zzywmcn0jv'\n",
    "            l.append(g[0])  # 将id值放入空列表中\n",
    "\n",
    "            # number of total actions\n",
    "            l.append(len(gr))  # 将id对应数据的长度放入列表\n",
    "\n",
    "            # action features 特征-用户行为\n",
    "            # 每个用户行为出现的次数，各个行为类型的数量，平均值以及标准差\n",
    "            c_act = [0] * len(f_act)\n",
    "            for i, v in enumerate(gr.action.values):  # i是从0-1对应的位置，v 是用户行为特征的值\n",
    "                c_act[f_act[v]] += 1\n",
    "            _, c_act_uqc = np.unique(gr.action.values, return_counts=True)\n",
    "            # 计算用户行为行为特征各个类型数量的长度，平均值以及标准差\n",
    "            c_act += [len(c_act_uqc), np.mean(c_act_uqc), np.std(c_act_uqc)]\n",
    "            l = l + c_act\n",
    "\n",
    "            # action_detail features 特征-用户行为具体\n",
    "            # (how many times each value occurs, numb of unique values, mean and std)\n",
    "            c_act_detail = [0] * len(f_act_detail)\n",
    "            for i, v in enumerate(gr.action_detail.values):\n",
    "                c_act_detail[f_act_detail[v]] += 1\n",
    "            _, c_act_det_uqc = np.unique(gr.action_detail.values, return_counts=True)\n",
    "            c_act_detail += [len(c_act_det_uqc), np.mean(c_act_det_uqc), np.std(c_act_det_uqc)]\n",
    "            l = l + c_act_detail\n",
    "\n",
    "            # action_type features  特征-用户行为类型 click等\n",
    "            # (how many times each value occurs, numb of unique values, mean and std\n",
    "            # + log of the sum of secs_elapsed for each value)\n",
    "            l_act_type = [0] * len(f_act_type)\n",
    "            c_act_type = [0] * len(f_act_type)\n",
    "            sev = gr.secs_elapsed.values\n",
    "            for i, v in enumerate(gr.action_type.values):\n",
    "                l_act_type[f_act_type[v]] += sev[i]  # sev = gr.secs_elapsed.fillna(0).values ，求每个行为类型总的停留时长\n",
    "                c_act_type[f_act_type[v]] += 1\n",
    "            l_act_type = np.log(1 + np.array(l_act_type)).tolist()  # 每个行为类型总的停留时长，差异比较大，进行log处理\n",
    "            _, c_act_type_uqc = np.unique(gr.action_type.values, return_counts=True)\n",
    "            c_act_type += [len(c_act_type_uqc), np.mean(c_act_type_uqc), np.std(c_act_type_uqc)]\n",
    "            l = l + c_act_type + l_act_type\n",
    "\n",
    "            # device_type features 特征-设备类型\n",
    "            # (how many times each value occurs, numb of unique values, mean and std)\n",
    "            c_dev_type = [0] * len(f_dev_type)\n",
    "            for i, v in enumerate(gr.device_type.values):\n",
    "                c_dev_type[f_dev_type[v]] += 1\n",
    "            c_dev_type.append(len(np.unique(gr.device_type.values)))\n",
    "            _, c_dev_type_uqc = np.unique(gr.device_type.values, return_counts=True)\n",
    "            c_dev_type += [len(c_dev_type_uqc), np.mean(c_dev_type_uqc), np.std(c_dev_type_uqc)]\n",
    "            l = l + c_dev_type\n",
    "\n",
    "            # secs_elapsed features  特征-停留时长\n",
    "            l_secs = [0] * 5\n",
    "            l_log = [0] * 15\n",
    "            if len(sev) > 0:\n",
    "                # Simple statistics about the secs_elapsed values.\n",
    "                l_secs[0] = np.log(1 + np.sum(sev))\n",
    "                l_secs[1] = np.log(1 + np.mean(sev))\n",
    "                l_secs[2] = np.log(1 + np.std(sev))\n",
    "                l_secs[3] = np.log(1 + np.median(sev))\n",
    "                l_secs[4] = l_secs[0] / float(l[1])  #\n",
    "\n",
    "                # Values are grouped in 15 intervals. Compute the number of values\n",
    "                # in each interval.\n",
    "                # sev = gr.secs_elapsed.fillna(0).values\n",
    "                log_sev = np.log(1 + sev).astype(int)\n",
    "                # np.bincount():Count number of occurrences of each value in array of non-negative ints.\n",
    "                l_log = np.bincount(log_sev, minlength=15).tolist()\n",
    "                print(\"\\r进度：%.2f%%\" % (float(k / len(dgr_sess) * 100)), end=' ')\n",
    "                k=k+1\n",
    "            l = l + l_secs + l_log\n",
    "\n",
    "            # The list l has the feature values of one sample.\n",
    "            samples.append(l)\n",
    "        print(\"\\n\")\n",
    "session_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing objects\n",
    "samples = np.array(samples)\n",
    "samp_ar = samples[:, 1:].astype(np.float16)  # 取除id外的特征数据\n",
    "samp_id = samples[:, 0]  # 取id，id位于第一列\n",
    "\n",
    "# 为提取的特征创建一个dataframe\n",
    "col_names = []  # name of the columns\n",
    "for i in range(len(samples[0]) - 1):  # 减1的原因是因为有个id\n",
    "    col_names.append('c_' + str(i))  # 起名字的方式\n",
    "df_agg_sess = pd.DataFrame(samp_ar, columns=col_names)\n",
    "df_agg_sess['id'] = samp_id\n",
    "df_agg_sess.index = df_agg_sess.id  # 将id作为index\n",
    "#经过特征提取后，session文件由6个特征变为584个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 trian和test文件进行特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算出train的行数，便于之后对train和test数据进行分离操作\n",
    "train_row = train.shape[0]\n",
    "# 预测值放入labels\n",
    "labels = train['信誉评级'].values\n",
    "weiyue = train['是否违约'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除预测值的列\n",
    "del train['信誉评级']\n",
    "del train['是否违约']\n",
    "del train['企业名称']\n",
    "del train['总销售额（含税）']\n",
    "del train['发票状态']\n",
    "del train['总票数']\n",
    "del train['废票数']\n",
    "del test['企业名称']\n",
    "del test['发票状态']\n",
    "del test['总票数']\n",
    "del test['废票数']\n",
    "# 合并，连接test 和 train，采用上下拼接\n",
    "df = pd.concat([train, test], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp_first_active特征\n",
    "# 转换为datetime类型\n",
    "# tfa = df.timestamp_first_active.astype(str).apply(lambda x: datetime.datetime(int(x[:4]),\n",
    "#                                                                           int(x[4:6]), \n",
    "#                                                                           int(x[6:8]),\n",
    "#                                                                           int(x[8:10]),\n",
    "#                                                                           int(x[10:12]),\n",
    "#                                                                           int(x[12:])))\n",
    "\n",
    "# datetime类对象有year，month，day等属性\n",
    "# df['tfa_year'] = np.array([x.year for x in tfa])\n",
    "# df['tfa_month'] = np.array([x.month for x in tfa])\n",
    "# df['tfa_day'] = np.array([x.day for x in tfa])\n",
    "# # 提取特征weekday\n",
    "# # isoweekday() 可以返回一周的星期几，e.g.星期日：0；星期一：1\n",
    "# df['tfa_wd'] = np.array([x.isoweekday() for x in tfa]) \n",
    "# df_tfa_wd = pd.get_dummies(df.tfa_wd, prefix = 'tfa_wd')  # one hot encoding 独热编码\n",
    "# df = pd.concat((df, df_tfa_wd), axis = 1) #添加df['tfa_wd'] 编码后的特征\n",
    "# df.drop(['tfa_wd'], axis = 1, inplace = True)#删除原有未编码的特征\n",
    "# # 提取特征季节\n",
    "# Y = 2000\n",
    "# seasons = [(0, (date(Y,  1,  1),  date(Y,  3, 20))),  #'winter'\n",
    "#            (1, (date(Y,  3, 21),  date(Y,  6, 20))),  #'spring'\n",
    "#            (2, (date(Y,  6, 21),  date(Y,  9, 22))),  #'summer'\n",
    "#            (3, (date(Y,  9, 23),  date(Y, 12, 20))),  #'autumn'\n",
    "#            (0, (date(Y, 12, 21),  date(Y, 12, 31)))]  #'winter'\n",
    "\n",
    "# def get_season(dt):\n",
    "#     dt = dt.date() #获取日期\n",
    "#     dt = dt.replace(year=Y) #将年统一换成2000年\n",
    "#     return next(season for season, (start, end) in seasons if start <= dt <= end)\n",
    "\n",
    "# df['tfa_season'] = np.array([get_season(x) for x in tfa])\n",
    "# df_tfa_season = pd.get_dummies(df.tfa_season, prefix = 'tfa_season') # one hot encoding 独热编码\n",
    "# df = pd.concat((df, df_tfa_season), axis = 1)\n",
    "# df.drop(['tfa_season'], axis = 1, inplace = True) # 删除原来列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_account_created特征提取\n",
    "# 将date_account_created转换为datetime类型\n",
    "# dac = pd.to_datetime(df.date_account_created)\n",
    "# # 提取年月日\n",
    "# df['dac_year'] = np.array([x.year for x in dac])\n",
    "# df['dac_month'] = np.array([x.month for x in dac])\n",
    "# df['dac_day'] = np.array([x.day for x in dac])\n",
    "# # 提取季节\n",
    "# df['dac_season'] = np.array([get_season(x) for x in dac])\n",
    "# df_dac_season = pd.get_dummies(df.dac_season, prefix = 'dac_season')\n",
    "# df = pd.concat((df, df_dac_season), axis = 1)\n",
    "# df.drop(['dac_season'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取用户在airbnb平台活跃到正式注册所花的时间，即date_account_created和timestamp_first_active之间的差值\n",
    "# dt_span = dac.subtract(tfa).dt.days\n",
    "# dt_span.value_counts().head(10)\n",
    "\n",
    "# def get_span(dt):\n",
    "#     # dt is an integer\n",
    "#     if dt == -1:\n",
    "#         return 'OneDay'\n",
    "#     elif (dt < 30) & (dt > -1):\n",
    "#         return 'OneMonth'\n",
    "#     elif (dt >= 30) & (dt <= 365):\n",
    "#         return 'OneYear'\n",
    "#     else:\n",
    "#         return 'other'\n",
    "\n",
    "# df['dt_span'] = np.array([get_span(x) for x in dt_span])\n",
    "# df_dt_span = pd.get_dummies(df.dt_span, prefix = 'dt_span') # 编码 one hot encoding \n",
    "# df = pd.concat((df, df_dt_span), axis = 1)\n",
    "# df.drop(['dt_span'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除提取特征后原来的列\n",
    "# df.drop(['date_account_created','timestamp_first_active'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>总销售额(不含税)</th>\n",
       "      <th>废票率</th>\n",
       "      <th>销售利润率</th>\n",
       "      <th>成本费用利润率</th>\n",
       "      <th>需求不稳定性</th>\n",
       "      <th>供给不稳定性</th>\n",
       "      <th>种类</th>\n",
       "      <th>销售额增长率</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>406584.330178</td>\n",
       "      <td>0.027620</td>\n",
       "      <td>-0.584294</td>\n",
       "      <td>-0.285225</td>\n",
       "      <td>0.493208</td>\n",
       "      <td>0.692083</td>\n",
       "      <td>0</td>\n",
       "      <td>0.295005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>59084.171279</td>\n",
       "      <td>0.082002</td>\n",
       "      <td>0.636450</td>\n",
       "      <td>3.309458</td>\n",
       "      <td>0.487079</td>\n",
       "      <td>0.648419</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>57017.799528</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>0.879740</td>\n",
       "      <td>-73.144618</td>\n",
       "      <td>2.888205</td>\n",
       "      <td>1.085142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.106139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>183996.968996</td>\n",
       "      <td>0.085164</td>\n",
       "      <td>0.622565</td>\n",
       "      <td>2651.411699</td>\n",
       "      <td>0.195853</td>\n",
       "      <td>0.631020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.610476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20263.228887</td>\n",
       "      <td>0.051887</td>\n",
       "      <td>-0.085710</td>\n",
       "      <td>0.252060</td>\n",
       "      <td>0.924635</td>\n",
       "      <td>0.457947</td>\n",
       "      <td>0</td>\n",
       "      <td>1.373607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>36643.928830</td>\n",
       "      <td>0.132129</td>\n",
       "      <td>-0.906883</td>\n",
       "      <td>0.601857</td>\n",
       "      <td>0.554781</td>\n",
       "      <td>0.765360</td>\n",
       "      <td>0</td>\n",
       "      <td>1.334760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>51295.062649</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>0.727744</td>\n",
       "      <td>12.559076</td>\n",
       "      <td>2.134448</td>\n",
       "      <td>1.334641</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>35888.883279</td>\n",
       "      <td>0.113103</td>\n",
       "      <td>0.467951</td>\n",
       "      <td>7.466405</td>\n",
       "      <td>0.513269</td>\n",
       "      <td>1.003465</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>31959.213536</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>0.921236</td>\n",
       "      <td>13.313790</td>\n",
       "      <td>0.456695</td>\n",
       "      <td>0.868553</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>34083.158447</td>\n",
       "      <td>0.091549</td>\n",
       "      <td>0.821057</td>\n",
       "      <td>2482.629634</td>\n",
       "      <td>0.442726</td>\n",
       "      <td>0.837902</td>\n",
       "      <td>0</td>\n",
       "      <td>3.303515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>14843.912714</td>\n",
       "      <td>0.059087</td>\n",
       "      <td>-0.243203</td>\n",
       "      <td>0.231873</td>\n",
       "      <td>1.252876</td>\n",
       "      <td>0.253241</td>\n",
       "      <td>0</td>\n",
       "      <td>0.424286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>22368.440309</td>\n",
       "      <td>0.077193</td>\n",
       "      <td>-0.436289</td>\n",
       "      <td>0.778858</td>\n",
       "      <td>0.257167</td>\n",
       "      <td>0.749670</td>\n",
       "      <td>0</td>\n",
       "      <td>4.665198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>21245.477101</td>\n",
       "      <td>0.149532</td>\n",
       "      <td>-2.623135</td>\n",
       "      <td>1.567708</td>\n",
       "      <td>0.632757</td>\n",
       "      <td>0.656238</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>20908.403179</td>\n",
       "      <td>0.071643</td>\n",
       "      <td>0.510905</td>\n",
       "      <td>1.749276</td>\n",
       "      <td>0.970399</td>\n",
       "      <td>0.560072</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>21182.433920</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.788547</td>\n",
       "      <td>24894.393197</td>\n",
       "      <td>0.143623</td>\n",
       "      <td>0.218404</td>\n",
       "      <td>0</td>\n",
       "      <td>2.471609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>20941.043550</td>\n",
       "      <td>0.111617</td>\n",
       "      <td>0.996129</td>\n",
       "      <td>656.645966</td>\n",
       "      <td>0.332993</td>\n",
       "      <td>0.556360</td>\n",
       "      <td>0</td>\n",
       "      <td>3.318431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>15498.077144</td>\n",
       "      <td>0.170118</td>\n",
       "      <td>-0.381105</td>\n",
       "      <td>1.147484</td>\n",
       "      <td>0.715401</td>\n",
       "      <td>0.524711</td>\n",
       "      <td>0</td>\n",
       "      <td>2.268534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>18772.647454</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.060385</td>\n",
       "      <td>5.689483</td>\n",
       "      <td>0.448993</td>\n",
       "      <td>0.450783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.819421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>18044.388607</td>\n",
       "      <td>0.089706</td>\n",
       "      <td>-0.070225</td>\n",
       "      <td>0.098763</td>\n",
       "      <td>0.367398</td>\n",
       "      <td>0.371509</td>\n",
       "      <td>0</td>\n",
       "      <td>0.160063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>19182.640360</td>\n",
       "      <td>0.051480</td>\n",
       "      <td>-0.067998</td>\n",
       "      <td>1.052645</td>\n",
       "      <td>0.531421</td>\n",
       "      <td>0.439835</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>11841.293823</td>\n",
       "      <td>0.025921</td>\n",
       "      <td>-1.019730</td>\n",
       "      <td>1.652838</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>0.307602</td>\n",
       "      <td>0</td>\n",
       "      <td>1.168016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>11193.399352</td>\n",
       "      <td>0.125480</td>\n",
       "      <td>0.047168</td>\n",
       "      <td>0.927381</td>\n",
       "      <td>0.227476</td>\n",
       "      <td>0.648028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.520215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>18323.893206</td>\n",
       "      <td>0.022969</td>\n",
       "      <td>0.044026</td>\n",
       "      <td>0.299379</td>\n",
       "      <td>0.420165</td>\n",
       "      <td>0.579994</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>14052.036766</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.309105</td>\n",
       "      <td>2.018491</td>\n",
       "      <td>0.266797</td>\n",
       "      <td>0.458211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.852353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>3799.703642</td>\n",
       "      <td>0.167689</td>\n",
       "      <td>0.501748</td>\n",
       "      <td>40.623161</td>\n",
       "      <td>0.488439</td>\n",
       "      <td>0.468478</td>\n",
       "      <td>0</td>\n",
       "      <td>2.165850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>3664.800349</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>-0.563012</td>\n",
       "      <td>-0.017145</td>\n",
       "      <td>0.210565</td>\n",
       "      <td>0.484326</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>4405.822050</td>\n",
       "      <td>0.053815</td>\n",
       "      <td>-0.171923</td>\n",
       "      <td>0.123139</td>\n",
       "      <td>0.639826</td>\n",
       "      <td>0.432735</td>\n",
       "      <td>0</td>\n",
       "      <td>0.555298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>4902.962543</td>\n",
       "      <td>0.077329</td>\n",
       "      <td>0.480859</td>\n",
       "      <td>94.737907</td>\n",
       "      <td>0.170931</td>\n",
       "      <td>0.581439</td>\n",
       "      <td>0</td>\n",
       "      <td>3.107142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>4517.143547</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.249748</td>\n",
       "      <td>1692.742862</td>\n",
       "      <td>0.068487</td>\n",
       "      <td>0.035841</td>\n",
       "      <td>0</td>\n",
       "      <td>0.733588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>7149.344769</td>\n",
       "      <td>0.066250</td>\n",
       "      <td>0.086860</td>\n",
       "      <td>209.782651</td>\n",
       "      <td>0.155807</td>\n",
       "      <td>0.168435</td>\n",
       "      <td>0</td>\n",
       "      <td>1.690974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>272</td>\n",
       "      <td>61.813035</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>-0.535289</td>\n",
       "      <td>0.682783</td>\n",
       "      <td>1.320822</td>\n",
       "      <td>0.432890</td>\n",
       "      <td>0</td>\n",
       "      <td>1.278144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>273</td>\n",
       "      <td>65.021496</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>35.269586</td>\n",
       "      <td>0.256917</td>\n",
       "      <td>0.024062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>274</td>\n",
       "      <td>61.886738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.114162</td>\n",
       "      <td>9.261968</td>\n",
       "      <td>0.063435</td>\n",
       "      <td>0.622558</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>275</td>\n",
       "      <td>44.216001</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-0.464201</td>\n",
       "      <td>17.031185</td>\n",
       "      <td>0.416782</td>\n",
       "      <td>0.103028</td>\n",
       "      <td>0</td>\n",
       "      <td>1.121049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>276</td>\n",
       "      <td>81.564797</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.440850</td>\n",
       "      <td>17.302229</td>\n",
       "      <td>0.251139</td>\n",
       "      <td>0.246954</td>\n",
       "      <td>0</td>\n",
       "      <td>0.622194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>277</td>\n",
       "      <td>31.403071</td>\n",
       "      <td>0.493506</td>\n",
       "      <td>-0.491587</td>\n",
       "      <td>4.387394</td>\n",
       "      <td>0.182390</td>\n",
       "      <td>0.530185</td>\n",
       "      <td>0</td>\n",
       "      <td>2.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>278</td>\n",
       "      <td>47.038958</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>-0.057150</td>\n",
       "      <td>0.269740</td>\n",
       "      <td>1.055649</td>\n",
       "      <td>0.279011</td>\n",
       "      <td>0</td>\n",
       "      <td>1.644575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>279</td>\n",
       "      <td>119.754835</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.855530</td>\n",
       "      <td>44.298526</td>\n",
       "      <td>0.517919</td>\n",
       "      <td>0.142570</td>\n",
       "      <td>0</td>\n",
       "      <td>0.503631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>280</td>\n",
       "      <td>83.029222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-0.527264</td>\n",
       "      <td>0.863058</td>\n",
       "      <td>0.349294</td>\n",
       "      <td>0.456730</td>\n",
       "      <td>0</td>\n",
       "      <td>1.334406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>281</td>\n",
       "      <td>59.143300</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.120182</td>\n",
       "      <td>0.653779</td>\n",
       "      <td>0.250112</td>\n",
       "      <td>0.345020</td>\n",
       "      <td>0</td>\n",
       "      <td>2.944978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>282</td>\n",
       "      <td>83.176644</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>-0.027110</td>\n",
       "      <td>7.933941</td>\n",
       "      <td>0.473309</td>\n",
       "      <td>0.445665</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.081137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>283</td>\n",
       "      <td>184.894465</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>-0.146657</td>\n",
       "      <td>8.059136</td>\n",
       "      <td>0.678507</td>\n",
       "      <td>0.280067</td>\n",
       "      <td>0</td>\n",
       "      <td>0.558317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>284</td>\n",
       "      <td>52.214294</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.122535</td>\n",
       "      <td>16.805084</td>\n",
       "      <td>0.678631</td>\n",
       "      <td>0.802066</td>\n",
       "      <td>0</td>\n",
       "      <td>1.109817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>285</td>\n",
       "      <td>21.643045</td>\n",
       "      <td>0.108911</td>\n",
       "      <td>0.457352</td>\n",
       "      <td>9.599281</td>\n",
       "      <td>0.419798</td>\n",
       "      <td>0.497322</td>\n",
       "      <td>0</td>\n",
       "      <td>1.901056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>286</td>\n",
       "      <td>40.696583</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.715334</td>\n",
       "      <td>19.711299</td>\n",
       "      <td>0.363997</td>\n",
       "      <td>1.445233</td>\n",
       "      <td>0</td>\n",
       "      <td>1.800358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>287</td>\n",
       "      <td>26.030524</td>\n",
       "      <td>0.309278</td>\n",
       "      <td>0.052348</td>\n",
       "      <td>2.594000</td>\n",
       "      <td>0.360257</td>\n",
       "      <td>0.330977</td>\n",
       "      <td>0</td>\n",
       "      <td>1.555031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>288</td>\n",
       "      <td>32.055278</td>\n",
       "      <td>0.080537</td>\n",
       "      <td>0.404260</td>\n",
       "      <td>3.595368</td>\n",
       "      <td>0.318526</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0</td>\n",
       "      <td>1.928542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>289</td>\n",
       "      <td>28.697887</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>-0.244849</td>\n",
       "      <td>18.678633</td>\n",
       "      <td>0.068843</td>\n",
       "      <td>0.470943</td>\n",
       "      <td>0</td>\n",
       "      <td>3.144522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>290</td>\n",
       "      <td>37.708737</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.266836</td>\n",
       "      <td>1.493633</td>\n",
       "      <td>0.311932</td>\n",
       "      <td>0.140038</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.300682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>291</td>\n",
       "      <td>8.532875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.876379</td>\n",
       "      <td>0.887306</td>\n",
       "      <td>0.103753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.222073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>292</td>\n",
       "      <td>20.941441</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.029091</td>\n",
       "      <td>0.041187</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.450264</td>\n",
       "      <td>0</td>\n",
       "      <td>1.264983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>293</td>\n",
       "      <td>16.166992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.759347</td>\n",
       "      <td>1.310778</td>\n",
       "      <td>0.141793</td>\n",
       "      <td>0.107198</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>294</td>\n",
       "      <td>42.583054</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>-0.143916</td>\n",
       "      <td>15.102560</td>\n",
       "      <td>0.301167</td>\n",
       "      <td>0.156736</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>295</td>\n",
       "      <td>5.207547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.659329</td>\n",
       "      <td>-0.996497</td>\n",
       "      <td>0.251022</td>\n",
       "      <td>0.835824</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.149123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>296</td>\n",
       "      <td>7.163027</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>-53.930823</td>\n",
       "      <td>-0.915866</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>0.999112</td>\n",
       "      <td>0</td>\n",
       "      <td>1.423286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>297</td>\n",
       "      <td>24.570076</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>-0.411722</td>\n",
       "      <td>1.879647</td>\n",
       "      <td>0.112779</td>\n",
       "      <td>0.549535</td>\n",
       "      <td>0</td>\n",
       "      <td>2.583839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>298</td>\n",
       "      <td>2.701831</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.661405</td>\n",
       "      <td>0.967029</td>\n",
       "      <td>0.148339</td>\n",
       "      <td>0.274148</td>\n",
       "      <td>0</td>\n",
       "      <td>1.697063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>299</td>\n",
       "      <td>7.278640</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.607978</td>\n",
       "      <td>1.139597</td>\n",
       "      <td>0</td>\n",
       "      <td>0.330052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>300</td>\n",
       "      <td>7.689642</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>-28.402030</td>\n",
       "      <td>-0.221047</td>\n",
       "      <td>0.123780</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0</td>\n",
       "      <td>17.042026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>301</td>\n",
       "      <td>8.512621</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>-0.742636</td>\n",
       "      <td>0.544303</td>\n",
       "      <td>0.152594</td>\n",
       "      <td>0.434552</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.590376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>425 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      总销售额(不含税)       废票率      销售利润率       成本费用利润率    需求不稳定性  \\\n",
       "0      0  406584.330178  0.027620  -0.584294     -0.285225  0.493208   \n",
       "1      1   59084.171279  0.082002   0.636450      3.309458  0.487079   \n",
       "2      2   57017.799528  0.015993   0.879740    -73.144618  2.888205   \n",
       "3      3  183996.968996  0.085164   0.622565   2651.411699  0.195853   \n",
       "4      4   20263.228887  0.051887  -0.085710      0.252060  0.924635   \n",
       "5      5   36643.928830  0.132129  -0.906883      0.601857  0.554781   \n",
       "6      6   51295.062649  0.014358   0.727744     12.559076  2.134448   \n",
       "7      7   35888.883279  0.113103   0.467951      7.466405  0.513269   \n",
       "8      8   31959.213536  0.024721   0.921236     13.313790  0.456695   \n",
       "9      9   34083.158447  0.091549   0.821057   2482.629634  0.442726   \n",
       "10    10   14843.912714  0.059087  -0.243203      0.231873  1.252876   \n",
       "11    11   22368.440309  0.077193  -0.436289      0.778858  0.257167   \n",
       "12    12   21245.477101  0.149532  -2.623135      1.567708  0.632757   \n",
       "13    13   20908.403179  0.071643   0.510905      1.749276  0.970399   \n",
       "14    14   21182.433920  0.063018   0.788547  24894.393197  0.143623   \n",
       "15    15   20941.043550  0.111617   0.996129    656.645966  0.332993   \n",
       "16    16   15498.077144  0.170118  -0.381105      1.147484  0.715401   \n",
       "17    17   18772.647454  0.094488   0.060385      5.689483  0.448993   \n",
       "18    18   18044.388607  0.089706  -0.070225      0.098763  0.367398   \n",
       "19    19   19182.640360  0.051480  -0.067998      1.052645  0.531421   \n",
       "20    20   11841.293823  0.025921  -1.019730      1.652838  0.175200   \n",
       "21    21   11193.399352  0.125480   0.047168      0.927381  0.227476   \n",
       "22    22   18323.893206  0.022969   0.044026      0.299379  0.420165   \n",
       "23    23   14052.036766  0.080808   0.309105      2.018491  0.266797   \n",
       "24    24    3799.703642  0.167689   0.501748     40.623161  0.488439   \n",
       "25    25    3664.800349  0.107143  -0.563012     -0.017145  0.210565   \n",
       "26    26    4405.822050  0.053815  -0.171923      0.123139  0.639826   \n",
       "27    27    4902.962543  0.077329   0.480859     94.737907  0.170931   \n",
       "28    28    4517.143547  0.012632   0.249748   1692.742862  0.068487   \n",
       "29    29    7149.344769  0.066250   0.086860    209.782651  0.155807   \n",
       "..   ...            ...       ...        ...           ...       ...   \n",
       "395  272      61.813035  0.071429  -0.535289      0.682783  1.320822   \n",
       "396  273      65.021496  0.278689   0.012997     35.269586  0.256917   \n",
       "397  274      61.886738  0.000000  -0.114162      9.261968  0.063435   \n",
       "398  275      44.216001  0.210526  -0.464201     17.031185  0.416782   \n",
       "399  276      81.564797  0.089552   0.440850     17.302229  0.251139   \n",
       "400  277      31.403071  0.493506  -0.491587      4.387394  0.182390   \n",
       "401  278      47.038958  0.120000  -0.057150      0.269740  1.055649   \n",
       "402  279     119.754835  0.128571   0.855530     44.298526  0.517919   \n",
       "403  280      83.029222  0.285714  -0.527264      0.863058  0.349294   \n",
       "404  281      59.143300  0.444444   0.120182      0.653779  0.250112   \n",
       "405  282      83.176644  0.225806  -0.027110      7.933941  0.473309   \n",
       "406  283     184.894465  0.295455  -0.146657      8.059136  0.678507   \n",
       "407  284      52.214294  0.187500   0.122535     16.805084  0.678631   \n",
       "408  285      21.643045  0.108911   0.457352      9.599281  0.419798   \n",
       "409  286      40.696583  0.015152   0.715334     19.711299  0.363997   \n",
       "410  287      26.030524  0.309278   0.052348      2.594000  0.360257   \n",
       "411  288      32.055278  0.080537   0.404260      3.595368  0.318526   \n",
       "412  289      28.697887  0.175000  -0.244849     18.678633  0.068843   \n",
       "413  290      37.708737  0.214286   0.266836      1.493633  0.311932   \n",
       "414  291       8.532875  0.000000   0.876379      0.887306  0.103753   \n",
       "415  292      20.941441  0.080000   0.029091      0.041187  0.016060   \n",
       "416  293      16.166992  0.000000  -0.759347      1.310778  0.141793   \n",
       "417  294      42.583054  0.130435  -0.143916     15.102560  0.301167   \n",
       "418  295       5.207547  0.000000  -4.659329     -0.996497  0.251022   \n",
       "419  296       7.163027  0.095238 -53.930823     -0.915866  0.229492   \n",
       "420  297      24.570076  0.034483  -0.411722      1.879647  0.112779   \n",
       "421  298       2.701831  0.100000   0.661405      0.967029  0.148339   \n",
       "422  299       7.278640  0.142857  -0.294118     -0.294118  0.607978   \n",
       "423  300       7.689642  0.139535 -28.402030     -0.221047  0.123780   \n",
       "424  301       8.512621  0.444444  -0.742636      0.544303  0.152594   \n",
       "\n",
       "       供给不稳定性  种类     销售额增长率  \n",
       "0    0.692083   0   0.295005  \n",
       "1    0.648419   0   0.376334  \n",
       "2    1.085142   0   0.106139  \n",
       "3    0.631020   0   0.610476  \n",
       "4    0.457947   0   1.373607  \n",
       "5    0.765360   0   1.334760  \n",
       "6    1.334641   0   0.077752  \n",
       "7    1.003465   0   0.127142  \n",
       "8    0.868553   0   0.079691  \n",
       "9    0.837902   0   3.303515  \n",
       "10   0.253241   0   0.424286  \n",
       "11   0.749670   0   4.665198  \n",
       "12   0.656238   0   0.252834  \n",
       "13   0.560072   0   0.089555  \n",
       "14   0.218404   0   2.471609  \n",
       "15   0.556360   0   3.318431  \n",
       "16   0.524711   0   2.268534  \n",
       "17   0.450783   0   0.819421  \n",
       "18   0.371509   0   0.160063  \n",
       "19   0.439835   0   0.348202  \n",
       "20   0.307602   0   1.168016  \n",
       "21   0.648028   0   0.520215  \n",
       "22   0.579994   0   0.024651  \n",
       "23   0.458211   0   0.852353  \n",
       "24   0.468478   0   2.165850  \n",
       "25   0.484326   0   0.923341  \n",
       "26   0.432735   0   0.555298  \n",
       "27   0.581439   0   3.107142  \n",
       "28   0.035841   0   0.733588  \n",
       "29   0.168435   0   1.690974  \n",
       "..        ...  ..        ...  \n",
       "395  0.432890   0   1.278144  \n",
       "396  0.024062   0   0.415037  \n",
       "397  0.622558   0   2.570643  \n",
       "398  0.103028   0   1.121049  \n",
       "399  0.246954   0   0.622194  \n",
       "400  0.530185   0   2.568700  \n",
       "401  0.279011   0   1.644575  \n",
       "402  0.142570   0   0.503631  \n",
       "403  0.456730   0   1.334406  \n",
       "404  0.345020   0   2.944978  \n",
       "405  0.445665   0  -0.081137  \n",
       "406  0.280067   0   0.558317  \n",
       "407  0.802066   0   1.109817  \n",
       "408  0.497322   0   1.901056  \n",
       "409  1.445233   0   1.800358  \n",
       "410  0.330977   0   1.555031  \n",
       "411  0.348900   0   1.928542  \n",
       "412  0.470943   0   3.144522  \n",
       "413  0.140038   0  -0.300682  \n",
       "414  0.000000   0   1.222073  \n",
       "415  0.450264   0   1.264983  \n",
       "416  0.107198   0   0.867669  \n",
       "417  0.156736   0   0.687643  \n",
       "418  0.835824   0  -0.149123  \n",
       "419  0.999112   0   1.423286  \n",
       "420  0.549535   0   2.583839  \n",
       "421  0.274148   0   1.697063  \n",
       "422  1.139597   0   0.330052  \n",
       "423  0.706006   0  17.042026  \n",
       "424  0.434552   0  -0.590376  \n",
       "\n",
       "[425 rows x 9 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对其余特征进行one hot encoding\n",
    "feat_toOHE = ['种类']\n",
    "for f in feat_toOHE:\n",
    "    df[f] = pd.get_dummies(df[f], prefix=f, dummy_na=True) # 缺失值单独成一类\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 整合数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gender  age  signup_method  signup_flow  language  affiliate_channel  \\\n",
      "0            1    1              0            1         0                  0   \n",
      "1            0    0              0            1         0                  0   \n",
      "2            0    0              1            0         0                  0   \n",
      "3            0    0              0            1         0                  0   \n",
      "4            1    0              1            1         0                  0   \n",
      "5            1    1              1            1         0                  0   \n",
      "6            0    0              1            1         0                  0   \n",
      "7            0    0              1            1         0                  0   \n",
      "8            0    0              1            1         0                  0   \n",
      "9            1    0              1            1         0                  0   \n",
      "10           0    0              1            1         0                  0   \n",
      "11           0    0              1            1         0                  0   \n",
      "12           1    1              1            1         0                  0   \n",
      "13           0    0              1            1         0                  0   \n",
      "14           0    0              1            1         0                  0   \n",
      "15           0    0              1            1         0                  0   \n",
      "16           1    1              1            1         0                  0   \n",
      "17           1    0              1            1         0                  0   \n",
      "18           1    1              1            1         0                  0   \n",
      "19           0    0              1            1         0                  0   \n",
      "20           1    1              1            1         0                  0   \n",
      "21           0    0              1            1         0                  0   \n",
      "22           1    0              1            1         0                  0   \n",
      "23           1    1              1            1         0                  0   \n",
      "24           0    0              1            1         0                  0   \n",
      "25           0    0              1            1         0                  0   \n",
      "26           1    1              1            1         0                  0   \n",
      "27           0    0              1            1         0                  0   \n",
      "28           1    0              1            1         0                  0   \n",
      "29           1    0              1            1         0                  0   \n",
      "...        ...  ...            ...          ...       ...                ...   \n",
      "275517       1    1              0            0         0                  0   \n",
      "275518       0    0              1            1         0                  0   \n",
      "275519       1    1              1            1         0                  0   \n",
      "275520       0    0              0            0         0                  0   \n",
      "275521       0    0              0            1         0                  0   \n",
      "275522       1    1              1            1         0                  0   \n",
      "275523       1    1              1            0         0                  0   \n",
      "275524       1    1              1            0         0                  0   \n",
      "275525       1    1              1            1         0                  0   \n",
      "275526       0    0              0            1         0                  0   \n",
      "275527       0    0              0            1         0                  0   \n",
      "275528       1    1              1            1         0                  0   \n",
      "275529       1    1              1            1         0                  0   \n",
      "275530       0    0              1            1         0                  0   \n",
      "275531       1    0              1            1         0                  0   \n",
      "275532       0    0              0            0         0                  0   \n",
      "275533       1    1              1            1         0                  0   \n",
      "275534       0    0              0            0         0                  0   \n",
      "275535       0    0              0            1         0                  0   \n",
      "275536       0    0              0            1         0                  0   \n",
      "275537       1    1              1            0         0                  0   \n",
      "275538       0    0              1            1         0                  0   \n",
      "275539       1    0              1            1         0                  0   \n",
      "275540       0    0              1            1         0                  0   \n",
      "275541       1    1              1            1         0                  0   \n",
      "275542       1    0              1            1         0                  0   \n",
      "275543       1    1              1            0         0                  0   \n",
      "275544       1    1              1            1         0                  0   \n",
      "275545       1    1              1            1         0                  0   \n",
      "275546       0    0              1            1         0                  0   \n",
      "\n",
      "        affiliate_provider  first_affiliate_tracked  signup_app  \\\n",
      "0                        0                        0           0   \n",
      "1                        0                        0           0   \n",
      "2                        0                        0           0   \n",
      "3                        0                        0           0   \n",
      "4                        0                        0           0   \n",
      "5                        0                        0           0   \n",
      "6                        0                        0           0   \n",
      "7                        0                        0           0   \n",
      "8                        0                        0           0   \n",
      "9                        0                        0           0   \n",
      "10                       0                        0           0   \n",
      "11                       0                        0           0   \n",
      "12                       0                        0           0   \n",
      "13                       0                        1           0   \n",
      "14                       0                        0           0   \n",
      "15                       0                        0           0   \n",
      "16                       0                        0           0   \n",
      "17                       0                        0           0   \n",
      "18                       0                        0           0   \n",
      "19                       0                        0           0   \n",
      "20                       0                        0           0   \n",
      "21                       0                        1           0   \n",
      "22                       0                        0           0   \n",
      "23                       0                        0           0   \n",
      "24                       0                        0           0   \n",
      "25                       0                        0           0   \n",
      "26                       0                        0           0   \n",
      "27                       0                        0           0   \n",
      "28                       0                        0           0   \n",
      "29                       0                        0           0   \n",
      "...                    ...                      ...         ...   \n",
      "275517                   0                        0           1   \n",
      "275518                   0                        0           0   \n",
      "275519                   0                        0           0   \n",
      "275520                   0                        0           0   \n",
      "275521                   0                        0           0   \n",
      "275522                   0                        0           0   \n",
      "275523                   0                        0           1   \n",
      "275524                   0                        0           1   \n",
      "275525                   0                        1           0   \n",
      "275526                   0                        0           0   \n",
      "275527                   0                        0           0   \n",
      "275528                   0                        0           0   \n",
      "275529                   0                        0           0   \n",
      "275530                   0                        1           0   \n",
      "275531                   0                        1           0   \n",
      "275532                   0                        0           1   \n",
      "275533                   0                        0           0   \n",
      "275534                   0                        0           1   \n",
      "275535                   0                        0           0   \n",
      "275536                   0                        1           0   \n",
      "275537                   0                        0           0   \n",
      "275538                   0                        0           0   \n",
      "275539                   0                        0           0   \n",
      "275540                   0                        1           0   \n",
      "275541                   0                        1           0   \n",
      "275542                   0                        0           0   \n",
      "275543                   0                        0           1   \n",
      "275544                   0                        1           0   \n",
      "275545                   0                        0           0   \n",
      "275546                   0                        0           0   \n",
      "\n",
      "        first_device_type    ...     c_574  c_575  c_576  c_577  c_578  c_579  \\\n",
      "0                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "1                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "2                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "3                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "4                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "5                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "6                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "7                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "8                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "9                       0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "10                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "11                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "12                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "13                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "14                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "15                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "16                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "17                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "18                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "19                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "20                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "21                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "22                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "23                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "24                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "25                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "26                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "27                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "28                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "29                      0    ...      -2.0   -2.0   -2.0   -2.0   -2.0   -2.0   \n",
      "...                   ...    ...       ...    ...    ...    ...    ...    ...   \n",
      "275517                  1    ...      25.0   14.0   15.0    5.0    4.0    0.0   \n",
      "275518                  0    ...       6.0    4.0   40.0   21.0   21.0    9.0   \n",
      "275519                  0    ...       1.0    4.0    3.0    4.0    3.0    1.0   \n",
      "275520                  0    ...       3.0    5.0    7.0   10.0    5.0    4.0   \n",
      "275521                  0    ...       0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "275522                  1    ...       1.0    1.0    4.0    8.0    0.0    0.0   \n",
      "275523                  1    ...      13.0   13.0    5.0    7.0    0.0    0.0   \n",
      "275524                  1    ...      35.0   36.0   48.0   12.0    5.0    1.0   \n",
      "275525                  0    ...       4.0    3.0    3.0    7.0    4.0    1.0   \n",
      "275526                  0    ...       7.0    6.0    2.0    4.0    5.0    1.0   \n",
      "275527                  0    ...       2.0    0.0    2.0    3.0    4.0    1.0   \n",
      "275528                  0    ...       3.0    5.0    2.0    4.0    0.0    3.0   \n",
      "275529                  0    ...       3.0    4.0    2.0    3.0    1.0    1.0   \n",
      "275530                  0    ...      41.0    8.0   30.0   34.0   18.0   16.0   \n",
      "275531                  0    ...      11.0   15.0    8.0   10.0    6.0    4.0   \n",
      "275532                  1    ...      17.0    8.0   12.0    8.0    5.0    3.0   \n",
      "275533                  0    ...       1.0    1.0    1.0    0.0    0.0    2.0   \n",
      "275534                  1    ...       1.0    2.0    4.0    2.0    1.0    0.0   \n",
      "275535                  0    ...      10.0    9.0   12.0    8.0    1.0    4.0   \n",
      "275536                  0    ...       5.0    3.0    4.0    5.0    5.0    1.0   \n",
      "275537                  0    ...      13.0    4.0    6.0    9.0   15.0    4.0   \n",
      "275538                  0    ...       3.0    3.0    4.0    4.0    3.0    4.0   \n",
      "275539                  0    ...      11.0   13.0   10.0    9.0    2.0    1.0   \n",
      "275540                  0    ...       4.0    8.0    9.0    4.0    2.0    0.0   \n",
      "275541                  0    ...      10.0    9.0    4.0    9.0    2.0    1.0   \n",
      "275542                  0    ...      13.0    7.0   21.0   27.0   12.0    0.0   \n",
      "275543                  1    ...       3.0    4.0    4.0    1.0    0.0    0.0   \n",
      "275544                  0    ...      12.0   13.0   15.0   13.0    4.0    1.0   \n",
      "275545                  0    ...       4.0    1.0    1.0    2.0    2.0    0.0   \n",
      "275546                  0    ...       2.0    2.0    1.0    2.0    3.0    1.0   \n",
      "\n",
      "        c_580  c_581  c_582  all_null  \n",
      "0        -2.0   -2.0   -2.0       583  \n",
      "1        -2.0   -2.0   -2.0       583  \n",
      "2        -2.0   -2.0   -2.0       583  \n",
      "3        -2.0   -2.0   -2.0       583  \n",
      "4        -2.0   -2.0   -2.0       583  \n",
      "5        -2.0   -2.0   -2.0       583  \n",
      "6        -2.0   -2.0   -2.0       583  \n",
      "7        -2.0   -2.0   -2.0       583  \n",
      "8        -2.0   -2.0   -2.0       583  \n",
      "9        -2.0   -2.0   -2.0       583  \n",
      "10       -2.0   -2.0   -2.0       583  \n",
      "11       -2.0   -2.0   -2.0       583  \n",
      "12       -2.0   -2.0   -2.0       583  \n",
      "13       -2.0   -2.0   -2.0       583  \n",
      "14       -2.0   -2.0   -2.0       583  \n",
      "15       -2.0   -2.0   -2.0       583  \n",
      "16       -2.0   -2.0   -2.0       583  \n",
      "17       -2.0   -2.0   -2.0       583  \n",
      "18       -2.0   -2.0   -2.0       583  \n",
      "19       -2.0   -2.0   -2.0       583  \n",
      "20       -2.0   -2.0   -2.0       583  \n",
      "21       -2.0   -2.0   -2.0       583  \n",
      "22       -2.0   -2.0   -2.0       583  \n",
      "23       -2.0   -2.0   -2.0       583  \n",
      "24       -2.0   -2.0   -2.0       583  \n",
      "25       -2.0   -2.0   -2.0       583  \n",
      "26       -2.0   -2.0   -2.0       583  \n",
      "27       -2.0   -2.0   -2.0       583  \n",
      "28       -2.0   -2.0   -2.0       583  \n",
      "29       -2.0   -2.0   -2.0       583  \n",
      "...       ...    ...    ...       ...  \n",
      "275517    0.0    0.0    1.0         0  \n",
      "275518    1.0    1.0    0.0         0  \n",
      "275519    0.0    0.0    0.0         0  \n",
      "275520    0.0    1.0    0.0         0  \n",
      "275521    0.0    0.0    0.0         0  \n",
      "275522    0.0    0.0    0.0         0  \n",
      "275523    1.0    0.0    0.0         0  \n",
      "275524    0.0    0.0    0.0         0  \n",
      "275525    1.0    0.0    0.0         0  \n",
      "275526    2.0    0.0    1.0         0  \n",
      "275527    0.0    0.0    0.0         0  \n",
      "275528    1.0    1.0    0.0         0  \n",
      "275529    2.0    0.0    0.0         0  \n",
      "275530    3.0    0.0    2.0         0  \n",
      "275531    1.0    0.0    0.0         0  \n",
      "275532    1.0    0.0    0.0         0  \n",
      "275533    1.0    0.0    0.0         0  \n",
      "275534    0.0    0.0    0.0         0  \n",
      "275535    1.0    0.0    0.0         0  \n",
      "275536    1.0    0.0    0.0         0  \n",
      "275537    0.0    0.0    0.0         0  \n",
      "275538    0.0    0.0    0.0         0  \n",
      "275539    0.0    0.0    0.0         0  \n",
      "275540    1.0    0.0    0.0         0  \n",
      "275541    1.0    0.0    0.0         0  \n",
      "275542    0.0    0.0    1.0         0  \n",
      "275543    0.0    0.0    0.0         0  \n",
      "275544    2.0    0.0    0.0         0  \n",
      "275545    1.0    0.0    0.0         0  \n",
      "275546    1.0    1.0    1.0         0  \n",
      "\n",
      "[275547 rows x 620 columns]\n"
     ]
    }
   ],
   "source": [
    "#将对session提取的特征整合到一起\n",
    "df_all = pd.merge(df, df_agg_sess, how='left')\n",
    "df_all = df_all.drop(['id'], axis=1) #删除id\n",
    "df_all = df_all.fillna(-2)  #对没有sesssion data的特征进行缺失值处理，值为负数说明是空\n",
    "#加了一列，表示每一行总共有多少空值，这也作为一个特征\n",
    "df_all['all_null'] = np.array([sum(r<0) for r in df_all.values]) # 值为负即为空值\n",
    "print(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不合并session文件\n",
    "df_all=df\n",
    "df_id=df_all['id'].values\n",
    "df_all = df_all.drop(['id'], axis=1) #删除id\n",
    "df_all = df_all.fillna(-2)  #对没有sesssion data的特征进行缺失值处理，值为负数说明是空\n",
    "#加了一列，表示每一行总共有多少空值，这也作为一个特征\n",
    "# df_all['all_null'] = np.array([sum(r<0) for r in df_all.values]) # 值为负即为空值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将train和test数据进行分离操作\n",
    "Xtrain = df_all.iloc[:train_row, :]\n",
    "Xtest = df_all.iloc[train_row:, :]\n",
    "testlabels = df_id[train_row:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [6] are constant.\n",
      "  UserWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x155388e4548>,\n",
       "  <matplotlib.axis.XTick at 0x155388e43c8>,\n",
       "  <matplotlib.axis.XTick at 0x155394f5388>,\n",
       "  <matplotlib.axis.XTick at 0x15539504188>,\n",
       "  <matplotlib.axis.XTick at 0x15539504f88>,\n",
       "  <matplotlib.axis.XTick at 0x1553891a748>,\n",
       "  <matplotlib.axis.XTick at 0x15538902788>],\n",
       " [Text(0, 0, '总销售额(不含税)'),\n",
       "  Text(0, 0, '废票率'),\n",
       "  Text(0, 0, '销售利润率'),\n",
       "  Text(0, 0, '成本费用利润率'),\n",
       "  Text(0, 0, '需求不稳定性'),\n",
       "  Text(0, 0, '供给不稳定性'),\n",
       "  Text(0, 0, '种类')])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24635 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 38144 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 21806 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 39069 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 19981 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 21547 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 31246 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24223 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 31080 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 29575 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 21033 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28070 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 25104 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 26412 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 36153 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 29992 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 38656 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 27714 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 31283 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 23450 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24615 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 20379 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 32473 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 31181 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 31867 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24635 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 38144 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 21806 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 39069 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 19981 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 21547 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 31246 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24223 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 31080 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 29575 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 21033 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28070 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 25104 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 26412 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 36153 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 29992 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 38656 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 27714 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 31283 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 23450 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24615 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 20379 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 32473 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 31181 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\37999\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 31867 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 特征选择\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "selector=SelectKBest(f_classif,k=5)\n",
    "selector.fit(Xtrain,labels)\n",
    "scores=-np.log10(selector.pvalues_)\n",
    "plt.bar(range(len(Xtrain.columns)),scores)\n",
    "plt.xticks(range(len(Xtrain.columns)),Xtrain.columns,rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes=['all_null','dac_year','first_browser','affiliate_provider','language']\n",
    "# df_all.drop(deletes,axis=1,inplace=True)\n",
    "# 将train和test数据进行分离操作\n",
    "Xtrain = df_all.iloc[:train_row, :]\n",
    "Xtest = df_all.iloc[train_row:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将提取的特征生成csv文件\n",
    "Xtrain.to_excel(\"xtrain_v2.xlsx\")\n",
    "Xtest.to_excel(\"xtest_v2.xlsx\")\n",
    "#labels.tofile（）：Write array to a file as text or binary (default)\n",
    "pd.DataFrame(labels).to_excel('ytrain_v2.xlsx', float_format='%s') #存放目标变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B    38\n",
      "C    34\n",
      "A    27\n",
      "D    24\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 读取特征文件\n",
    "xtrain = pd.read_excel(\"xtrain_v2.xlsx\",index_col=0)\n",
    "ytrain = pd.read_excel(\"ytrain_v2.xlsx\", index_col=0)\n",
    "xtrain.head()\n",
    "ytrain.head()\n",
    "print(ytrain[0].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# 将目标变量进行labels encoding\n",
    "le = LabelEncoder()\n",
    "ytrain_le = le.fit_transform(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取30%的数据进行模型训练\n",
    "# Let us take 10% of the data for faster training. \n",
    "n = int(xtrain.shape[0]*0.3)\n",
    "xtrain_new = xtrain.iloc[:n, :]  #训练数据\n",
    "ytrain_new = ytrain_le[:n]       #训练数据的目标变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化数据集\n",
    "X_scaler = StandardScaler()\n",
    "xtrain_new = X_scaler.fit_transform(xtrain_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评分模型\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def dcg_score(y_true, y_score, k=5):\n",
    "    \"\"\"\n",
    "    y_true : array, shape = [n_samples] #数据\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array, shape = [n_samples, n_classes] #预测的分数\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1] #分数从大到小的索引号\n",
    "    y_true = np.take(y_true, order[:k]) #取出前k[0,k）个分数索引号\n",
    "    \n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)# 有误\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "def ndcg_score(ground_truth, predictions, k=5):   \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : array, shape = [n_samples]\n",
    "        Ground truth (true labels represended as integers).\n",
    "    predictions : array, shape = [n_samples, n_classes]\n",
    "        Predicted probabilities. 预测的概率\n",
    "    k : int\n",
    "        Rank.\n",
    "    \"\"\"\n",
    "    # 标签二值化\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(len(predictions) + 1))\n",
    "    T = lb.transform(ground_truth)\n",
    "    scores = []\n",
    "    # 迭代每个真实值并计算dcg分数\n",
    "    for y_true, y_score in zip(T, predictions):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        score = float(actual) / float(best)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 树模型\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "LEARNING_RATE = 0.01\n",
    "N_ESTIMATORS = 50\n",
    "RANDOM_STATE = 2017\n",
    "MAX_DEPTH = 7\n",
    "k_ndcg = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#建了一个tree字典\n",
    "clf_tree ={\n",
    "    'DTree': DecisionTreeClassifier(max_depth=MAX_DEPTH,\n",
    "                                    random_state=RANDOM_STATE),\n",
    "    \n",
    "    'RF': RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
    "                                 max_depth=MAX_DEPTH,\n",
    "                                 random_state=RANDOM_STATE),\n",
    "    \n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=N_ESTIMATORS,\n",
    "                                   learning_rate=LEARNING_RATE,\n",
    "                                   random_state=RANDOM_STATE),\n",
    "    \n",
    "    'Bagging': BaggingClassifier(n_estimators=N_ESTIMATORS,\n",
    "                                 random_state=RANDOM_STATE),\n",
    "    \n",
    "    'ExtraTree': ExtraTreesClassifier(max_depth=MAX_DEPTH,\n",
    "                                      n_estimators=N_ESTIMATORS,\n",
    "                                      random_state=RANDOM_STATE),\n",
    "    \n",
    "    'GraBoost': GradientBoostingClassifier(learning_rate=LEARNING_RATE,\n",
    "                                           max_depth=MAX_DEPTH,\n",
    "                                           n_estimators=N_ESTIMATORS,\n",
    "                                           random_state=RANDOM_STATE)\n",
    "}\n",
    "train_score = []\n",
    "cv_score = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "k_ndcg = 3\n",
    "\n",
    "for key in clf_tree.keys():\n",
    "    \n",
    "    clf = clf_tree.get(key)\n",
    "    \n",
    "    train_score_iter = []\n",
    "    cv_score_iter = []\n",
    "\n",
    "    for train_index, test_index in kf.split(xtrain_new, ytrain_new):\n",
    "        # 训练集分成训练和验证\n",
    "        X_train, X_test = xtrain_new[train_index, :], xtrain_new[test_index, :]\n",
    "        y_train, y_test = ytrain_new[train_index], ytrain_new[test_index]\n",
    "        # 训练模型\n",
    "        clf.fit(X_train, y_train)\n",
    "        # 预测结果并评估\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        train_ndcg_score = ndcg_score(y_train, clf.predict_proba(X_train), k = k_ndcg)\n",
    "        cv_ndcg_score = ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "        # 评估结果保存，最后要取平均值作为总评估分数\n",
    "        train_score_iter.append(train_ndcg_score)\n",
    "        cv_score_iter.append(cv_ndcg_score)\n",
    "    # 每种算法的评估分数均保存\n",
    "    train_score.append(np.mean(train_score_iter))\n",
    "    cv_score.append(np.mean(cv_score_iter))\n",
    "\n",
    "train_score_tree = train_score\n",
    "cv_score_tree = cv_score\n",
    "\n",
    "ymin = np.min(cv_score)-0.05\n",
    "ymax = np.max(train_score)+0.05\n",
    "\n",
    "x_ticks = clf_tree.keys()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(x_ticks)), train_score_tree, 'ro-', label = 'training')\n",
    "plt.plot(range(len(x_ticks)), cv_score_tree, 'bo-', label = 'Cross-validation')\n",
    "\n",
    "plt.xticks(range(len(x_ticks)),x_ticks,rotation = 45, fontsize = 10)\n",
    "plt.xlabel(\"Tree method\", fontsize = 12)\n",
    "plt.ylabel(\"Score\", fontsize = 12)\n",
    "plt.xlim(-0.5, 5.5)\n",
    "plt.ylim(ymin, ymax)\n",
    "\n",
    "plt.legend(loc = 'best', fontsize = 12)\n",
    "plt.title(\"Different tree methods\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义评估指标\n",
    "def customized_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    top = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        top.append(np.argsort(preds[i])[::-1][:3]) # 将概率矩阵中每个数据预测的概率最高的五个类别索引号提出来\n",
    "    mat = np.reshape(np.repeat(labels,np.shape(top)[1]) == np.array(top).ravel(),np.array(top).shape).astype(int) # 将top变成一个布尔矩阵，每行代表一个数据，每行只有一个1，其中1在第一列说明预测很好\n",
    "    score = np.mean(np.sum(mat/np.log2(np.arange(2, mat.shape[1] + 2)),axis = 1)) # 列上乘以权重后每行中求和，得出该条数据的一个预测分数，再求总的平均数即为全部得分平均分\n",
    "    return 'ndcg3', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    top = []\n",
    "    train_predict = np.array([])\n",
    "    train_right = np.array([])\n",
    "    for i in range(preds.shape[0]):\n",
    "#         top.append(np.argsort(preds[i])[::-1][:1]) # 将概率矩阵中每个数据预测的概率最高的类别索引号提出来\n",
    "        train_predict_label = np.argmax(preds[i])\n",
    "        train_predict = np.append(train_predict, train_predict_label)\n",
    "#         train_right_label = np.argmax(labels[i])\n",
    "        train_right_label = labels[i]\n",
    "        train_right = np.append(train_right, train_right_label)\n",
    "        score = accuracy_score(train_right, train_predict)\n",
    "#     mat = np.reshape(np.repeat(labels,np.shape(top)[1]) == np.array(top).ravel(),np.array(top).shape).astype(int) # 将top变成一个布尔矩阵，每行代表一个数据，每行只有一个1，其中1在第一列说明预测很好\n",
    "#     score = np.mean(np.sum(mat/np.log2(np.arange(2, mat.shape[1] + 2)),axis = 1)) # 列上乘以权重后每行中求和，得出该条数据的一个预测分数，再求总的平均数即为全部得分平均分\n",
    "    return 'ndcg1', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# 解释ndcg\n",
    "a=np.array([[1,0,0,0,0],\n",
    "          [0,0,0,0,0],\n",
    "          [0,0,1,0,0],\n",
    "          [1,0,0,0,0],\n",
    "          [0,0,1,0,0]])\n",
    "s2 = np.mean(s1)\n",
    "s1=np.sum(s,axis = 1)\n",
    "s=a/np.log2(np.arange(2, 7))\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost原生接口\n",
    "import xgboost as xgb\n",
    "# xgboost parameters\n",
    "params={\n",
    "    'max_depth':3,# 构建树的深度，越大越容易过拟合\n",
    "    'eta':0.11,# 如同学习率\n",
    "    'num_round':20,#决策树数量\n",
    "    'objective':'multi:softprob',# 多分类的问题\n",
    "    'booster':'gbtree',\n",
    "    'num_class':4,# 类别数，与 multisoftmax 并用\n",
    "    'n_jobs':4,\n",
    "    'gamma':0.8,# 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "    'min_child_weight':1,# 叶子节点最小权重\n",
    "    'subsample':0.7,# 随机采样训练样本\n",
    "    'tree_method':'auto',# 树生成算法，自动选取最佳\n",
    "    'colsample_bytree':0.7,# 生成树时进行的列采样,和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)\n",
    "    'seed':RANDOM_STATE# 随机种子\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集整理\n",
    "X_train, X_test, y_train, y_test = train_test_split(xtrain_new,ytrain_new,test_size=0.2,random_state=RANDOM_STATE)\n",
    "train_xgb = xgb.DMatrix(X_train, label= y_train)\n",
    "test_xgb = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-ndcg3' will be used for early stopping.\n",
      "\n",
      "Will train until test-ndcg3 hasn't improved in 100 rounds.\n",
      "[0]\ttrain-merror:0.32095+0.07514\ttest-merror:0.57333+0.22050\ttrain-ndcg3:0.83640+0.03944\ttest-ndcg3:0.64809+0.14760\n",
      "[1]\ttrain-merror:0.28617+0.02650\ttest-merror:0.64667+0.17839\ttrain-ndcg3:0.86743+0.01605\ttest-ndcg3:0.66007+0.06236\n",
      "[2]\ttrain-merror:0.27747+0.03801\ttest-merror:0.54667+0.18571\ttrain-ndcg3:0.87246+0.02740\ttest-ndcg3:0.65841+0.06972\n",
      "[3]\ttrain-merror:0.26838+0.03217\ttest-merror:0.58667+0.18451\ttrain-ndcg3:0.88269+0.01126\ttest-ndcg3:0.64888+0.07835\n",
      "[4]\ttrain-merror:0.25968+0.03831\ttest-merror:0.58000+0.15720\ttrain-ndcg3:0.88590+0.01632\ttest-ndcg3:0.65134+0.08353\n",
      "[5]\ttrain-merror:0.25929+0.02145\ttest-merror:0.61333+0.11274\ttrain-ndcg3:0.88585+0.01634\ttest-ndcg3:0.65047+0.09083\n",
      "[6]\ttrain-merror:0.25929+0.03587\ttest-merror:0.58000+0.11662\ttrain-ndcg3:0.88698+0.01991\ttest-ndcg3:0.67944+0.07361\n",
      "[7]\ttrain-merror:0.24071+0.03313\ttest-merror:0.58000+0.11662\ttrain-ndcg3:0.89503+0.01904\ttest-ndcg3:0.66801+0.07716\n",
      "[8]\ttrain-merror:0.24980+0.01890\ttest-merror:0.54667+0.15290\ttrain-ndcg3:0.89287+0.01560\ttest-ndcg3:0.69174+0.07903\n",
      "[9]\ttrain-merror:0.25020+0.04689\ttest-merror:0.54667+0.15290\ttrain-ndcg3:0.89277+0.02851\ttest-ndcg3:0.69174+0.07903\n",
      "[10]\ttrain-merror:0.24071+0.04305\ttest-merror:0.54667+0.15290\ttrain-ndcg3:0.89394+0.02094\ttest-ndcg3:0.69174+0.07903\n",
      "[11]\ttrain-merror:0.22253+0.06053\ttest-merror:0.54667+0.15290\ttrain-ndcg3:0.89745+0.02599\ttest-ndcg3:0.71174+0.05761\n",
      "[12]\ttrain-merror:0.21383+0.04144\ttest-merror:0.51333+0.17588\ttrain-ndcg3:0.90500+0.02086\ttest-ndcg3:0.70404+0.09674\n",
      "[13]\ttrain-merror:0.21383+0.04144\ttest-merror:0.51333+0.17588\ttrain-ndcg3:0.90066+0.02164\ttest-ndcg3:0.72404+0.07707\n",
      "[14]\ttrain-merror:0.20514+0.03467\ttest-merror:0.51333+0.17588\ttrain-ndcg3:0.89952+0.02219\ttest-ndcg3:0.74404+0.06021\n",
      "[15]\ttrain-merror:0.19605+0.03362\ttest-merror:0.51333+0.17588\ttrain-ndcg3:0.90836+0.01997\ttest-ndcg3:0.76071+0.07025\n",
      "[16]\ttrain-merror:0.19605+0.03362\ttest-merror:0.51333+0.17588\ttrain-ndcg3:0.90401+0.02394\ttest-ndcg3:0.76071+0.07025\n",
      "[17]\ttrain-merror:0.20514+0.03467\ttest-merror:0.51333+0.17588\ttrain-ndcg3:0.90066+0.02483\ttest-ndcg3:0.76071+0.07025\n",
      "[18]\ttrain-merror:0.19605+0.03362\ttest-merror:0.51333+0.17588\ttrain-ndcg3:0.90401+0.02394\ttest-ndcg3:0.76595+0.06843\n",
      "[19]\ttrain-merror:0.19605+0.03362\ttest-merror:0.51333+0.17588\ttrain-ndcg3:0.90401+0.02394\ttest-ndcg3:0.76595+0.06843\n",
      "train-merror-mean    0.237866\n",
      "train-merror-std     0.037832\n",
      "test-merror-mean     0.550000\n",
      "test-merror-std      0.164546\n",
      "train-ndcg3-mean     0.890799\n",
      "train-ndcg3-std      0.021906\n",
      "test-ndcg3-mean      0.701890\n",
      "test-ndcg3-std       0.077975\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# cv调参\n",
    "cv_result = xgb.cv(params,\n",
    "                   train_xgb,\n",
    "                   num_boost_round=params['num_round'],\n",
    "                   nfold=5,\n",
    "                   feval = customized_eval,\n",
    "                   early_stopping_rounds=100,\n",
    "                   callbacks=[xgb.callback.early_stop(100),\n",
    "                              xgb.callback.print_evaluation(period=1,show_stdv=True)])\n",
    "print(np.mean(cv_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_X)):\n",
    "    predict_label = np.argmax(classifier.predict(test_X[i]))\n",
    "    test_predict = np.append(test_predict, predict_label)\n",
    "    right_label = np.argmax(y_test[i])\n",
    "    test_right = np.append(test_right, right_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x155366f4048>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "watchlist = [ (train_xgb,'train'), (test_xgb, 'test') ]\n",
    "# 预测模型评估，调整参数\n",
    "test_ndcg_score=[]\n",
    "for i in np.arange(0.5,0.01,1):\n",
    "    params['gamma']=i\n",
    "    # 训练模型\n",
    "    xgb2 = xgb.train(params,\n",
    "                train_xgb,\n",
    "                params['num_round'],\n",
    "                watchlist,\n",
    "                feval = customized_eval,# 自定义的评估函数\n",
    "                verbose_eval = 3,\n",
    "                early_stopping_rounds = 5)\n",
    "    y_pred = np.array(xgb2.predict(test_xgb))\n",
    "    test_ndcg_score.append(ndcg_score(y_test, y_pred, k=k_ndcg))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot()\n",
    "plt.plot(np.arange(0.5,0.01,1),test_ndcg_score, 'bo-', label = 'eta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:50:57] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.35714\ttest-merror:0.75000\ttrain-ndcg3:0.79124\ttest-ndcg3:0.59523\n",
      "Multiple eval metrics have been passed: 'test-ndcg3' will be used for early stopping.\n",
      "\n",
      "Will train until test-ndcg3 hasn't improved in 10 rounds.\n",
      "[3]\ttrain-merror:0.28571\ttest-merror:0.87500\ttrain-ndcg3:0.86734\ttest-ndcg3:0.64433\n",
      "[6]\ttrain-merror:0.28571\ttest-merror:0.87500\ttrain-ndcg3:0.86734\ttest-ndcg3:0.64433\n",
      "[9]\ttrain-merror:0.21429\ttest-merror:0.87500\ttrain-ndcg3:0.88903\ttest-ndcg3:0.64433\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.35714\ttest-merror:0.75000\ttrain-ndcg3:0.79124\ttest-ndcg3:0.59523\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got multiclass instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-66281d5a8b21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 预测模型评估，调整参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_xgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtest_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndcg_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_ndcg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mndcg_score\u001b[1;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1418\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1419\u001b[1;33m     \u001b[0m_check_dcg_target_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1420\u001b[0m     \u001b[0mgain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ndcg_sample_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_ties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_ties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36m_check_dcg_target_type\u001b[1;34m(y_true)\u001b[0m\n\u001b[0;32m   1161\u001b[0m         raise ValueError(\n\u001b[0;32m   1162\u001b[0m             \"Only {} formats are supported. Got {} instead\".format(\n\u001b[1;32m-> 1163\u001b[1;33m                 supported_fmt, y_type))\n\u001b[0m\u001b[0;32m   1164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got multiclass instead"
     ]
    }
   ],
   "source": [
    "watchlist = [ (train_xgb,'train'), (test_xgb, 'test') ]\n",
    "# 训练模型\n",
    "xgb2 = xgb.train(params,\n",
    "                train_xgb,\n",
    "                params['num_round'],\n",
    "                watchlist,\n",
    "                feval = customized_eval,# 自定义的评估函数\n",
    "                verbose_eval = 3,\n",
    "                early_stopping_rounds = 10)\n",
    "# 预测模型评估，调整参数\n",
    "y_pred = np.array(xgb2.predict(test_xgb))\n",
    "test_score=ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "print('test_score',test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'xtest_v2.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-f83ac3875a5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 预测结果输出文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mxtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xtest_v2.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# 标准化数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'xtest_v2.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# 预测结果输出文件\n",
    "xtest = pd.read_csv(\"xtest_v2.csv\",index_col=0)\n",
    "# 标准化数据集\n",
    "X_scaler = StandardScaler()\n",
    "xtest = X_scaler.fit_transform(xtest)\n",
    "\n",
    "realtest_xgb = xgb.DMatrix(xtest)\n",
    "predictions = np.array(xgb2.predict(realtest_xgb)) # predictions是一个概率矩阵，将概率最高的视为输出\n",
    "\n",
    "output=[]\n",
    "for i in range(predictions.shape[0]):\n",
    "    output.append(np.argsort(predictions[i])[::-1][:1])\n",
    "# 逆变换\n",
    "output=le.inverse_transform(output)\n",
    "\n",
    "d={\"id\" : testlabels,\n",
    "   \"country\" : output}\n",
    "pd_data = pd.DataFrame(d)\n",
    "pd_data.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost的sklearn接口\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection,metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据集整理\n",
    "X_train, X_test, y_train, y_test = train_test_split(xtrain_new,ytrain_new,test_size=0.2,random_state=RANDOM_STATE)\n",
    "train_xgb = xgb.DMatrix(X_train, label= y_train)\n",
    "test_xgb = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softprob',\n",
       " 'base_score': None,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': 0.7,\n",
       " 'gamma': 0.2,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 70,\n",
       " 'n_jobs': 4,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 0.8,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': False,\n",
       " 'verbosity': None,\n",
       " 'silent': False,\n",
       " 'num_class': 12,\n",
       " 'seed': 2017}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost的sklearn接口\n",
    "xgb1 = XGBClassifier(max_depth=7,# 构建树的深度，越大越容易过拟合\n",
    "                     learning_rate=0.11,# 如同学习率\n",
    "                     n_estimators=50,#决策树数量\n",
    "                     silent=False,\n",
    "                     objective='multi:softprob',# 多分类的问题\n",
    "                     booster='gbtree',\n",
    "                     num_class=12,# 类别数，与 multisoftmax 并用\n",
    "                     n_jobs=4,\n",
    "                     gamma=0.8,# 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "                     min_child_weight=1,\n",
    "                     subsample=0.8,# 随机采样训练样本\n",
    "                     colsample_bytree=0.7,# 生成树时进行的列采样\n",
    "                     seed=RANDOM_STATE)# 随机种子\n",
    "xgb1.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-ndcg5' will be used for early stopping.\n",
      "\n",
      "Will train until test-ndcg5 hasn't improved in 5 rounds.\n",
      "[0]\ttrain-merror:0.42651+0.00579\ttest-merror:0.42859+0.00950\ttrain-ndcg5:0.79662+0.00181\ttest-ndcg5:0.79331+0.00400\n",
      "[1]\ttrain-merror:0.42251+0.00336\ttest-merror:0.42529+0.00797\ttrain-ndcg5:0.79902+0.00101\ttest-ndcg5:0.79499+0.00352\n",
      "[2]\ttrain-merror:0.41923+0.00256\ttest-merror:0.42250+0.00608\ttrain-ndcg5:0.80060+0.00100\ttest-ndcg5:0.79619+0.00271\n",
      "[3]\ttrain-merror:0.41845+0.00211\ttest-merror:0.42157+0.00499\ttrain-ndcg5:0.80122+0.00060\ttest-ndcg5:0.79670+0.00269\n",
      "[4]\ttrain-merror:0.41674+0.00063\ttest-merror:0.42032+0.00420\ttrain-ndcg5:0.80204+0.00049\ttest-ndcg5:0.79710+0.00230\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.42651+0.00579\ttest-merror:0.42859+0.00950\ttrain-ndcg5:0.79662+0.00181\ttest-ndcg5:0.79331+0.00400\n",
      "\n",
      "   train-merror-mean  train-merror-std  test-merror-mean  test-merror-std  \\\n",
      "0           0.426505          0.005791          0.428593         0.009502   \n",
      "\n",
      "   train-ndcg5-mean  train-ndcg5-std  test-ndcg5-mean  test-ndcg5-std  \n",
      "0          0.796617         0.001813          0.79331        0.004002  \n"
     ]
    }
   ],
   "source": [
    "# cv调参\n",
    "cv_result = xgb.cv(xgb1.get_xgb_params(),\n",
    "                   train_xgb,\n",
    "                   num_boost_round=xgb1.get_xgb_params()['n_estimators'],\n",
    "                   nfold=5,\n",
    "                   #metrics='mlogloss',\n",
    "                   feval = customized_eval,\n",
    "                   early_stopping_rounds=5,\n",
    "                   callbacks=[xgb.callback.early_stop(5),\n",
    "                              xgb.callback.print_evaluation(period=1,show_stdv=True)])\n",
    "print(cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.44146\tvalidation_1-merror:0.44007\tvalidation_0-ndcg5:0.79095\tvalidation_1-ndcg5:0.79193\n",
      "Multiple eval metrics have been passed: 'validation_1-ndcg5' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-ndcg5 hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-merror:0.42846\tvalidation_1-merror:0.42641\tvalidation_0-ndcg5:0.79675\tvalidation_1-ndcg5:0.79692\n",
      "[2]\tvalidation_0-merror:0.41991\tvalidation_1-merror:0.41899\tvalidation_0-ndcg5:0.80013\tvalidation_1-ndcg5:0.79949\n",
      "[3]\tvalidation_0-merror:0.42342\tvalidation_1-merror:0.42063\tvalidation_0-ndcg5:0.79914\tvalidation_1-ndcg5:0.79908\n",
      "[4]\tvalidation_0-merror:0.41778\tvalidation_1-merror:0.41766\tvalidation_0-ndcg5:0.80116\tvalidation_1-ndcg5:0.80059\n",
      "[5]\tvalidation_0-merror:0.41719\tvalidation_1-merror:0.41634\tvalidation_0-ndcg5:0.80139\tvalidation_1-ndcg5:0.80123\n",
      "Stopping. Best iteration:\n",
      "[0]\tvalidation_0-merror:0.44146\tvalidation_1-merror:0.44007\tvalidation_0-ndcg5:0.79095\tvalidation_1-ndcg5:0.79193\n",
      "\n",
      "0.0030452096509721244\n"
     ]
    }
   ],
   "source": [
    "# 预测模型评估\n",
    "xgb_bst1 = xgb1.fit(X_train,\n",
    "                    y_train,\n",
    "                    eval_set=[(X_train,y_train),(X_test, y_test)],\n",
    "                    eval_metric=customized_eval,# 自定义的评估函数\n",
    "                   early_stopping_rounds = 5)\n",
    "y_pred = np.array(xgb_bst1.predict(X_test))\n",
    "test_ndcg_score = ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "print(test_ndcg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
